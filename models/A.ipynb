{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef1295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training ---\n",
      "Processing 465 images from 'images'...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "# --- Configuration ---\n",
    "# As per requirement 2, an 8x8 grid on an 800x600 image.\n",
    "GRID_W = 100  # 800 / 8\n",
    "GRID_H = 75   # 600 / 8\n",
    "TARGET_ASPECT_RATIO = 4.0 / 3.0\n",
    "\n",
    "# LBP parameters for texture analysis\n",
    "LBP_RADIUS = 1\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "\n",
    "# --- Image Preprocessing (Requirement 1) ---\n",
    "def process_image_for_training(path: str) -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Loads an image and processes it according to the strict 4:3 / 800x600 rules.\n",
    "    - Enforces 4:3 aspect ratio by center-cropping.\n",
    "    - Scales down large images.\n",
    "    - Discards small images.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read image at {path}\")\n",
    "        return None\n",
    "    \n",
    "    h, w, _ = img.shape\n",
    "    current_aspect_ratio = w / h\n",
    "    \n",
    "    # a. Crop to 4:3 aspect ratio if it's not already correct\n",
    "    if not np.isclose(current_aspect_ratio, TARGET_ASPECT_RATIO):\n",
    "        if current_aspect_ratio > TARGET_ASPECT_RATIO: # Image is wider than 4:3\n",
    "            new_w = int(TARGET_ASPECT_RATIO * h)\n",
    "            x_start = (w - new_w) // 2\n",
    "            img = img[:, x_start:x_start + new_w]\n",
    "        else: # Image is taller than 4:3\n",
    "            new_h = int(w / TARGET_ASPECT_RATIO)\n",
    "            y_start = (h - new_h) // 2\n",
    "            img = img[y_start:y_start + new_h, :]\n",
    "    \n",
    "    # b. Scale down if larger than 800x600\n",
    "    h, w, _ = img.shape\n",
    "    if w > 800 or h > 600:\n",
    "        img = cv2.resize(img, (800, 600), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # c. Do not scale up if smaller than 800x600 (discard)\n",
    "    h, w, _ = img.shape\n",
    "    if w < 800 or h < 600:\n",
    "        # print(f\"Info: Discarding small image: {path} ({w}x{h})\")\n",
    "        return None\n",
    "        \n",
    "    # Convert to grayscale for feature extraction\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# --- Feature Extraction (Same as before, but on processed images) ---\n",
    "def extract_features_from_cell(cell: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Extracts features from a single 100x75 grid cell.\"\"\"\n",
    "    features = []\n",
    "    features.append(np.mean(cell))\n",
    "    features.append(np.std(cell))\n",
    "    lbp = local_binary_pattern(cell, LBP_POINTS, LBP_RADIUS, method=\"uniform\")\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, LBP_POINTS + 3), density=True)\n",
    "    features.extend(hist)\n",
    "    edges = cv2.Canny(cell, 50, 150)\n",
    "    features.append(np.sum(edges > 0) / cell.size)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_features_from_image(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Extracts features from all 64 cells in a single image.\"\"\"\n",
    "    features_list = []\n",
    "    for y in range(0, img.shape[0], GRID_H):\n",
    "        for x in range(0, img.shape[1], GRID_W):\n",
    "            cell = img[y:y+GRID_H, x:x+GRID_W]\n",
    "            features_list.append(extract_features_from_cell(cell))\n",
    "    return np.vstack(features_list)\n",
    "\n",
    "# --- Main Training Pipeline ---\n",
    "if __name__ == \"__main__\":\n",
    "    training_folder = \"images\"\n",
    "    model_output_path = \"wildlife_detector_model.pkl\"\n",
    "    \n",
    "    os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "    print(\"--- Starting Model Training ---\")\n",
    "    image_paths = [os.path.join(training_folder, f) for f in os.listdir(training_folder) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"Error: No images found in '{training_folder}'. Please add images to train the model.\")\n",
    "    else:\n",
    "        all_features = []\n",
    "        print(f\"Processing {len(image_paths)} images from '{training_folder}'...\")\n",
    "        for path in image_paths:\n",
    "            processed_img = process_image_for_training(path)\n",
    "            if processed_img is not None:\n",
    "                features = extract_features_from_image(processed_img)\n",
    "                all_features.append(features)\n",
    "\n",
    "        if not all_features:\n",
    "            print(\"Error: No valid images found for training after processing. Check image dimensions.\")\n",
    "        else:\n",
    "            X_train = np.vstack(all_features)\n",
    "            print(f\"Successfully extracted {X_train.shape[0]} feature vectors for training.\")\n",
    "            \n",
    "            # 1. Create and fit the scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            \n",
    "            # 2. Create and fit the Isolation Forest model\n",
    "            # Contamination is the expected % of cells containing wildlife. Adjust if needed.\n",
    "            iso_forest = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)\n",
    "            iso_forest.fit(X_train_scaled)\n",
    "            \n",
    "            # 3. Save both models to a single pickle file\n",
    "            models = {'scaler': scaler, 'iso_forest': iso_forest}\n",
    "            joblib.dump(models, model_output_path)\n",
    "            \n",
    "            print(\"\\n--- Training Complete ---\")\n",
    "            print(f\"Scaler and Isolation Forest models have been saved to '{model_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "# --- Configuration (Must match the model's training script) ---\n",
    "GRID_W = 100\n",
    "GRID_H = 75\n",
    "TARGET_ASPECT_RATIO = 4.0 / 3.0\n",
    "LBP_RADIUS = 1\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "MODEL_PATH = \"wildlife_detector_model.pkl\"\n",
    "IMAGE_FOLDER = \"images\"\n",
    "\n",
    "# --- Helper Functions (Copied from generate_output.py) ---\n",
    "\n",
    "def process_image_for_prediction(path: str) -> np.ndarray | None:\n",
    "    \"\"\"Loads and processes an image using the same rules as training.\"\"\"\n",
    "    img = cv2.imread(path)\n",
    "    if img is None: return None\n",
    "    h, w, _ = img.shape\n",
    "    current_aspect_ratio = w / h\n",
    "    if not np.isclose(current_aspect_ratio, TARGET_ASPECT_RATIO):\n",
    "        if current_aspect_ratio > TARGET_ASPECT_RATIO:\n",
    "            new_w = int(TARGET_ASPECT_RATIO * h)\n",
    "            x_start = (w - new_w) // 2\n",
    "            img = img[:, x_start:x_start + new_w]\n",
    "        else:\n",
    "            new_h = int(w / TARGET_ASPECT_RATIO)\n",
    "            y_start = (h - new_h) // 2\n",
    "            img = img[y_start:y_start + new_h, :]\n",
    "    h, w, _ = img.shape\n",
    "    if w > 800 or h > 600:\n",
    "        img = cv2.resize(img, (800, 600), interpolation=cv2.INTER_AREA)\n",
    "    h, w, _ = img.shape\n",
    "    if w < 800 or h < 600: return None\n",
    "    return img\n",
    "\n",
    "def extract_features_from_image(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts features from all 64 cells in a single image.\n",
    "    img: Input image (assumed to be in BGR format).\n",
    "    Returns a 64xN feature array, where N is the number of features per cell\n",
    "    \"\"\"\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    features_list = []\n",
    "    for y in range(0, gray_img.shape[0], GRID_H):\n",
    "        for x in range(0, gray_img.shape[1], GRID_W):\n",
    "            cell = gray_img[y:y+GRID_H, x:x+GRID_W]\n",
    "            # --- Feature Extraction Logic (must be identical to training) ---\n",
    "            cell_features = []\n",
    "            cell_features.append(np.mean(cell))\n",
    "            cell_features.append(np.std(cell))\n",
    "            lbp = local_binary_pattern(cell, LBP_POINTS, LBP_RADIUS, method=\"uniform\")\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, LBP_POINTS + 3), density=True)\n",
    "            cell_features.extend(hist)\n",
    "            edges = cv2.Canny(cell, 50, 150)\n",
    "            cell_features.append(np.sum(edges > 0) / cell.size)\n",
    "            features_list.append(np.array(cell_features))\n",
    "    return np.vstack(features_list)\n",
    "\n",
    "def draw_grid_visualization(img: np.ndarray, grid_map: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Draws the final visualization.\"\"\"\n",
    "    def apply_dither_effect(cell: np.ndarray) -> np.ndarray:\n",
    "        h, w, _ = cell.shape\n",
    "        overlay = np.zeros_like(cell, dtype=np.uint8)\n",
    "        for y in range(0, h, 4):\n",
    "            for x in range(0, w, 4):\n",
    "                cv2.circle(overlay, (x, y), 1, (200, 200, 200), -1)\n",
    "        return cv2.addWeighted(cell, 0.5, overlay, 0.5, 0)\n",
    "\n",
    "    cell_number = 1\n",
    "    for i in range(8): # 8 rows\n",
    "        for j in range(8): # 8 cols\n",
    "            y1, x1 = i * GRID_H, j * GRID_W\n",
    "            y2, x2 = y1 + GRID_H, x1 + GRID_W\n",
    "            if grid_map[i, j] == 1:\n",
    "                img[y1:y2, x1:x2] = apply_dither_effect(img[y1:y2, x1:x2])\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 255), 1)\n",
    "            cv2.putText(img, str(cell_number), (x1 + 3, y1 + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)\n",
    "            cell_number += 1\n",
    "    return img\n",
    "\n",
    "# --- Main Test Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Check for model and image folder\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model file not found at '{MODEL_PATH}'. Please run train_model.py first.\")\n",
    "    elif not os.path.isdir(IMAGE_FOLDER):\n",
    "        print(f\"Error: Folder '{IMAGE_FOLDER}' not found. Please create it and add images.\")\n",
    "    else:\n",
    "        # 2. Find a random image\n",
    "        all_images = [f for f in os.listdir(IMAGE_FOLDER) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        if not all_images:\n",
    "            print(f\"No images found in '{IMAGE_FOLDER}'.\")\n",
    "        else:\n",
    "            random_image_name = random.choice(all_images)\n",
    "            image_path = os.path.join(IMAGE_FOLDER, random_image_name)\n",
    "            print(f\"--- Testing random image: {random_image_name} ---\")\n",
    "\n",
    "            # 3. Load models\n",
    "            models = joblib.load(MODEL_PATH)\n",
    "            scaler = models['scaler']\n",
    "            iso_forest = models['iso_forest']\n",
    "\n",
    "            # 4. Process the image\n",
    "            processed_img = process_image_for_prediction(image_path)\n",
    "            if processed_img is None:\n",
    "                print(\"Image was discarded as it did not meet the size/aspect ratio requirements.\")\n",
    "            else:\n",
    "                # 5. Extract features, scale, and predict\n",
    "                features = extract_features_from_image(processed_img)\n",
    "                features_scaled = scaler.transform(features)\n",
    "                preds = iso_forest.predict(features_scaled)\n",
    "\n",
    "                # 6. Visualize and show the result\n",
    "                grid_map = (preds == -1).astype(int).reshape((8, 8))\n",
    "                final_image = draw_grid_visualization(processed_img.copy(), grid_map)\n",
    "\n",
    "                # Save the output for review\n",
    "                output_filename = f\"test_output_{random_image_name}\"\n",
    "                cv2.imwrite(output_filename, final_image)\n",
    "                print(f\"Output saved to '{output_filename}'\")\n",
    "                \n",
    "                # Display in a window\n",
    "                cv2.imshow(f\"Test Result: {random_image_name}\", final_image)\n",
    "                print(\"Press any key to close the image window.\")\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "# ... (All your helper functions remain exactly the same) ...\n",
    "# process_image_for_prediction()\n",
    "# extract_features_from_image()\n",
    "# draw_grid_visualization()\n",
    "\n",
    "\n",
    "# --- Main Test Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Check for model and image folder\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model file not found at '{MODEL_PATH}'. Please run train_model.py first.\")\n",
    "    elif not os.path.isdir(IMAGE_FOLDER):\n",
    "        print(f\"Error: Folder '{IMAGE_FOLDER}' not found. Please create it and add images.\")\n",
    "    else:\n",
    "        # 2. Find a random image\n",
    "        all_images = [f for f in os.listdir(IMAGE_FOLDER) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        if not all_images:\n",
    "            print(f\"No images found in '{IMAGE_FOLDER}'.\")\n",
    "        else:\n",
    "            random_image_name = random.choice(all_images)\n",
    "            image_path = os.path.join(IMAGE_FOLDER, random_image_name)\n",
    "            print(f\"--- Testing random image: {random_image_name} ---\")\n",
    "\n",
    "            # 3. Load models\n",
    "            models = joblib.load(MODEL_PATH)\n",
    "            scaler = models['scaler']\n",
    "            \n",
    "            # === MODIFICATION 1 ===\n",
    "            # Load the model generically, whatever it is\n",
    "            model = models['model']\n",
    "            model_name = models.get('model_name', 'unknown') # .get() for backward compatibility\n",
    "            print(f\"Successfully loaded model: {model_name}\")\n",
    "            # ========================\n",
    "\n",
    "            # 4. Process the image\n",
    "            processed_img = process_image_for_prediction(image_path)\n",
    "            if processed_img is None:\n",
    "                print(\"Image was discarded as it did not meet the size/aspect ratio requirements.\")\n",
    "            else:\n",
    "                # 5. Extract features, scale, and predict\n",
    "                features = extract_features_from_image(processed_img)\n",
    "                features_scaled = scaler.transform(features)\n",
    "                \n",
    "                # === MODIFICATION 2 ===\n",
    "                # Use the generic 'model' variable to predict\n",
    "                preds = model.predict(features_scaled)\n",
    "                # ========================\n",
    "\n",
    "                # 6. Visualize and show the result\n",
    "                # This logic is correct: all 3 models use -1 for anomalies\n",
    "                grid_map = (preds == -1).astype(int).reshape((8, 8))\n",
    "                final_image = draw_grid_visualization(processed_img.copy(), grid_map)\n",
    "\n",
    "                # Save the output for review\n",
    "                output_filename = f\"test_output_{random_image_name}\"\n",
    "                cv2.imwrite(output_filename, final_image)\n",
    "                print(f\"Output saved to '{output_filename}'\")\n",
    "                \n",
    "                # Display in a window\n",
    "                cv2.imshow(f\"Test Result: {random_image_name} (Model: {model_name})\", final_image)\n",
    "                print(\"Press any key to close the image window.\")\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976a8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
